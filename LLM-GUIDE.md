# ğŸ¤– LLM (Language Models) - Guide Complet

## ğŸ“Œ Vue d'Ensemble

**LLM** = **Language Learning Model** (ModÃ¨les de Langage)

Ce projet utilise une **architecture modulaire d'accÃ¨s Ã  plusieurs modÃ¨les d'IA**:
- ğŸ”µ **Claude 3.5 Sonnet** (Anthropic) - Pour la planification
- ğŸŸ  **DeepSeek** - Pour la gÃ©nÃ©ration de code
- âœ… Extensible pour ajouter d'autres fournisseurs

**Localisation:** `project/lib/modules/llm/`

---

## ğŸ—ï¸ Architecture LLM

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Application Next.js                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PromptAgent  â”‚  CodeAgent  â”‚  ChatbotPanelâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   LLMManager        â”‚
        â”‚  (Singleton)        â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“             â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Anthropicâ”‚ â”‚ DeepSeek â”‚ â”‚Other Providerâ”‚
â”‚Provider â”‚ â”‚ Provider â”‚ â”‚(extensible) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“             â†“              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Claude   â”‚ â”‚DeepSeek  â”‚ â”‚Future Models â”‚
â”‚Models   â”‚ â”‚Models    â”‚ â”‚(GPT, etc)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Structure des Fichiers

```
lib/modules/llm/
â”œâ”€â”€ types.ts                    # Types TypeScript
â”œâ”€â”€ manager.ts                  # ğŸ¯ Gestionnaire central (Singleton)
â”œâ”€â”€ base-provider.ts            # Classe abstraite pour les fournisseurs
â”œâ”€â”€ anthropic-provider.ts       # Fournisseur Anthropic (Claude)
â””â”€â”€ providers/
    â”œâ”€â”€ anthropic.ts            # ImplÃ©mentation Anthropic
    â””â”€â”€ deepseek.ts             # ImplÃ©mentation DeepSeek
```

---

## ğŸ¯ Composant Central: LLMManager

### Qu'est-ce que LLMManager?

**LLMManager** est un **gestionnaire centralisÃ©** qui :
- ğŸ”‘ GÃ¨re toutes les clÃ©s API
- ğŸ“¦ Enregistre les fournisseurs (Anthropic, DeepSeek, etc.)
- ğŸ”„ Route les requÃªtes vers le bon fournisseur
- ğŸ“‹ Maintient une liste de tous les modÃ¨les disponibles
- ğŸ›ï¸ CrÃ©e des instances de modÃ¨les

### Pattern: Singleton

```typescript
// Toujours une seule instance dans toute l'application
const manager = LLMManager.getInstance();
```

**Avantage:** Pas de duplication, gestion centralisÃ©e

### API Principale

```typescript
// 1. RÃ©cupÃ©rer le gestionnaire (Singleton)
const manager = LLMManager.getInstance();

// 2. Enregistrer un fournisseur
manager.registerProvider(new CustomProvider());

// 3. Obtenir tous les modÃ¨les
const models = manager.getModelList();
// RÃ©sultat: [
//   { name: 'claude-3-5-sonnet', label: 'Claude 3.5', provider: 'Anthropic' },
//   { name: 'deepseek-chat', label: 'DeepSeek Chat', provider: 'DeepSeek' }
// ]

// 4. Appeler un modÃ¨le
const response = await manager.callModel(
  'Anthropic',              // Fournisseur
  'claude-3-5-sonnet-latest', // ModÃ¨le
  'Bonjour, qui es-tu?',   // Prompt
  'Tu es un assistant IA'  // SystÃ¨me (optionnel)
);
```

### Code DÃ©taillÃ©

```typescript
export class LLMManager {
  // Instance unique (Singleton)
  private static _instance: LLMManager;
  
  // Map des fournisseurs
  private _providers: Map<string, BaseProvider> = new Map();
  
  // Liste consolidÃ©e des modÃ¨les
  private _modelList: ModelInfo[] = [];

  /**
   * RÃ©cupÃ¨re l'instance unique
   */
  static getInstance(): LLMManager {
    if (!LLMManager._instance) {
      LLMManager._instance = new LLMManager();
    }
    return LLMManager._instance;
  }

  /**
   * Initialise les fournisseurs par dÃ©faut
   */
  private _registerProviders() {
    this.registerProvider(new AnthropicProvider());
    this.registerProvider(new DeepseekProvider());
  }

  /**
   * Enregistre un fournisseur
   */
  registerProvider(provider: BaseProvider) {
    if (this._providers.has(provider.name)) {
      console.warn(`${provider.name} already registered`);
      return;
    }
    this._providers.set(provider.name, provider);
    this._modelList = [...this._modelList, ...provider.staticModels];
  }

  /**
   * Appelle un modÃ¨le
   */
  async callModel(
    providerName: string,
    modelName: string,
    prompt: string,
    system?: string
  ): Promise<string> {
    const modelInstance = await this.getModelInstance(providerName, modelName);
    return await modelInstance.generate(prompt, system);
  }
}
```

---

## ğŸ“¦ Fournisseurs: Anthropic vs DeepSeek

### 1ï¸âƒ£ Anthropic Provider (Claude)

**Fichier:** `anthropic-provider.ts`

#### ModÃ¨les Disponibles

```typescript
staticModels = [
  {
    name: 'claude-3-5-sonnet-latest',
    label: 'Claude 3.5 Sonnet (latest)',
    provider: 'Anthropic',
    maxTokenAllowed: 8000,
  },
  {
    name: 'claude-3-5-haiku-latest',
    label: 'Claude 3.5 Haiku (latest)',
    provider: 'Anthropic',
    maxTokenAllowed: 8000,
  },
  // ... autres modÃ¨les
]
```

#### Configuration

```typescript
config = {
  apiTokenKey: 'CLAUDE_API_KEY',  // Variable d'environnement
};

getApiKeyLink = 'https://console.anthropic.com/settings/keys';
```

#### Appel API

```typescript
async generate(prompt: string, system?: string) {
  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': apiKey,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model: 'claude-3-5-sonnet-latest',
      max_tokens: 2048,
      messages: [
        ...(system ? [{ role: 'system', content: system }] : []),
        { role: 'user', content: prompt }
      ]
    })
  });

  const data = await response.json();
  return data.content[0].text;  // RÃ©ponse gÃ©nÃ©rale
}
```

#### Utilisation dans le Projet

```typescript
// PromptAgent utilise Claude pour la planification
const plan = await manager.callModel(
  'Anthropic',
  'claude-3-5-sonnet-latest',
  "Je veux une app e-commerce",
  "Tu es un architecte logiciel expert..."
);
```

---

### 2ï¸âƒ£ DeepSeek Provider

**Fichier:** `providers/deepseek.ts`

#### ModÃ¨les Disponibles

```typescript
staticModels: ModelInfo[] = [
  {
    name: 'deepseek-chat',
    label: 'DeepSeek Chat',
    provider: 'DeepSeek',
    maxTokenAllowed: 8000,
  },
  {
    name: 'deepseek-coder',
    label: 'DeepSeek Coder',
    provider: 'DeepSeek',
    maxTokenAllowed: 8000,
  },
];
```

#### Configuration

```typescript
config = {
  apiTokenKey: 'DEEPSEEK_API_KEY',  // Variable d'environnement
};

getApiKeyLink = 'https://platform.deepseek.com/console/api-keys';
```

#### Appel API

```typescript
async generateText({
  prompt,
  model = 'deepseek-chat',
  apiKey,
  system
}: {
  prompt: string;
  model?: string;
  apiKey: string;
  system?: string;
}): Promise<string> {
  const response = await fetch('https://api.deepseek.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`,
    },
    body: JSON.stringify({
      model,
      max_tokens: 2048,
      messages: [
        ...(system ? [{ role: 'system', content: system }] : []),
        { role: 'user', content: prompt }
      ]
    })
  });

  const data = await response.json();
  return data.choices[0].message.content;  // RÃ©ponse gÃ©nÃ©rÃ©e
}
```

#### Utilisation dans le Projet

```typescript
// CodeAgent utilise DeepSeek pour la gÃ©nÃ©ration de code
const code = await manager.callModel(
  'DeepSeek',
  'deepseek-coder',
  "GÃ©nÃ¨re un composant React Button avec Tailwind CSS",
  "Tu es un expert en React..."
);
```

---

## ğŸ”‘ ClÃ©s API et Variables d'Environnement

### Configuration

**Fichier:** `.env.local` (Ã  crÃ©er)

```bash
# Anthropic (Claude)
CLAUDE_API_KEY=sk-ant-v0-xxxxxxxxxxxxx

# DeepSeek
DEEPSEEK_API_KEY=sk-xxxxxxxxxxxxxxxxxx
```

### Comment obtenir les clÃ©s

#### 1. Claude (Anthropic)
1. Aller Ã : https://console.anthropic.com/settings/keys
2. CrÃ©er une nouvelle clÃ©
3. Copier la clÃ©
4. Ajouter Ã  `.env.local`: `CLAUDE_API_KEY=sk-ant-...`

#### 2. DeepSeek
1. Aller Ã : https://platform.deepseek.com/console/api-keys
2. CrÃ©er une nouvelle clÃ©
3. Copier la clÃ©
4. Ajouter Ã  `.env.local`: `DEEPSEEK_API_KEY=sk-...`

---

## ğŸ“‹ DiffÃ©rences: Claude vs DeepSeek

| Aspect | Claude 3.5 | DeepSeek |
|--------|-----------|----------|
| **SpÃ©cialitÃ©** | Planification, analyse | GÃ©nÃ©ration de code |
| **Vitesse** | Moyenne-rapide | Rapide |
| **CoÃ»t** | ModÃ©rÃ© | TrÃ¨s bon marchÃ© |
| **QualitÃ© Code** | Bonne | Excellente |
| **ModÃ¨les Dynamiques** | âœ… API pour lister | âŒ Statiques seulement |
| **Max Tokens** | 8000+ | 8000 |

### Quand utiliser?

```typescript
// Planification, architecture â†’ Claude
const plan = await callModel('Anthropic', 'claude-3-5-sonnet-latest', prompt, system);

// GÃ©nÃ©ration de code â†’ DeepSeek
const code = await callModel('DeepSeek', 'deepseek-coder', prompt, system);

// Chat/Questions â†’ Claude
const answer = await callModel('Anthropic', 'claude-3-5-haiku-latest', prompt);
```

---

## ğŸ’» Exemple Complet: Utiliser LLM

### ScÃ©nario: GÃ©nÃ©rer un Plan de Projet

```typescript
import { LLMManager } from '@/lib/modules/llm/manager';

async function createProject(userDescription: string) {
  // 1. Obtenir le gestionnaire
  const manager = LLMManager.getInstance();

  // 2. DÃ©finir le prompt systÃ¨me
  const systemPrompt = `
    Tu es un architecte logiciel expert.
    Analyse la description et crÃ©e un plan dÃ©taillÃ© en JSON.
    Inclus: stack, features, architecture, steps, files
  `;

  try {
    // 3. Appeler Claude pour obtenir le plan
    const planText = await manager.callModel(
      'Anthropic',                    // Fournisseur
      'claude-3-5-sonnet-latest',     // ModÃ¨le
      `CrÃ©e un plan pour: ${userDescription}`,  // Prompt
      systemPrompt                    // SystÃ¨me
    );

    // 4. Parser le JSON
    const plan = JSON.parse(planText);

    // 5. Pour chaque fichier du plan, appeler DeepSeek
    const generatedFiles = [];
    for (const file of plan.files) {
      const code = await manager.callModel(
        'DeepSeek',           // Fournisseur
        'deepseek-coder',     // ModÃ¨le
        `GÃ©nÃ¨re le code pour: ${file.description}`,
        'Tu es un expert en code'
      );

      generatedFiles.push({
        path: file.path,
        content: code
      });
    }

    return {
      plan,
      generatedFiles
    };
  } catch (error) {
    console.error('Erreur:', error);
    throw error;
  }
}

// Utilisation
const project = await createProject(
  'Application de chat avec Firebase et React'
);
```

---

## ğŸ—ï¸ BaseProvider: CrÃ©er un Nouveau Fournisseur

### Comment ajouter OpenAI?

```typescript
// lib/modules/llm/providers/openai.ts

import { BaseProvider } from '../base-provider';
import type { ModelInfo } from '../types';

export default class OpenAIProvider extends BaseProvider {
  name = 'OpenAI';
  getApiKeyLink = 'https://platform.openai.com/api-keys';

  config = {
    apiTokenKey: 'OPENAI_API_KEY',
  };

  staticModels: ModelInfo[] = [
    {
      name: 'gpt-4-turbo',
      label: 'GPT-4 Turbo',
      provider: 'OpenAI',
      maxTokenAllowed: 128000,
    },
    {
      name: 'gpt-3.5-turbo',
      label: 'GPT-3.5 Turbo',
      provider: 'OpenAI',
      maxTokenAllowed: 4096,
    },
  ];

  async getDynamicModels(): Promise<ModelInfo[]> {
    // Optionnel: rÃ©cupÃ©rer les modÃ¨les dynamiquement
    return this.staticModels;
  }

  getModelInstance(options: { model: string; serverEnv?: any; apiKeys?: Record<string, string> }) {
    const { apiKey } = this.getProviderBaseUrlAndKey({
      apiKeys: options.apiKeys,
      serverEnv: options.serverEnv,
      defaultBaseUrlKey: '',
      defaultApiTokenKey: 'OPENAI_API_KEY',
    });

    return {
      async generate(prompt: string, system?: string) {
        const response = await fetch('https://api.openai.com/v1/chat/completions', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`,
          },
          body: JSON.stringify({
            model: options.model,
            max_tokens: 2048,
            messages: [
              ...(system ? [{ role: 'system', content: system }] : []),
              { role: 'user', content: prompt }
            ]
          })
        });

        const data = await response.json();
        return data.choices[0].message.content;
      }
    };
  }
}
```

### Enregistrer le nouveau fournisseur

```typescript
// lib/modules/llm/manager.ts

private _registerProviders() {
  this.registerProvider(new AnthropicProvider());
  this.registerProvider(new DeepseekProvider());
  this.registerProvider(new OpenAIProvider());  // âœ… Ajouter ici
}
```

---

## ğŸ”„ Flux Complet: De l'Agent au ModÃ¨le

```
User Input
    â†“
"GÃ©nÃ¨re une app"
    â†“
PromptAgent()
    â”œâ”€ Construit un prompt systÃ¨me
    â”œâ”€ Appelle manager.callModel('Anthropic', 'claude-3-5-sonnet', prompt, system)
    â”‚
    â””â”€ LLMManager.callModel()
        â”œâ”€ getModelInstance('Anthropic', 'claude-3-5-sonnet')
        â”‚   â””â”€ RÃ©cupÃ¨re apiKey depuis process.env.CLAUDE_API_KEY
        â”‚
        â””â”€ modelInstance.generate(prompt, system)
            â”œâ”€ Construit la requÃªte HTTP
            â”œâ”€ Envoie Ã  https://api.anthropic.com/v1/messages
            â”œâ”€ ReÃ§oit la rÃ©ponse JSON
            â””â”€ Retourne data.content[0].text
    â†“
Plan structurÃ© reÃ§u
    â†“
CodeAgent()
    â”œâ”€ Pour chaque fichier du plan
    â”œâ”€ Appelle manager.callModel('DeepSeek', 'deepseek-coder', prompt, system)
    â”‚
    â””â”€ LLMManager.callModel()
        â”œâ”€ getModelInstance('DeepSeek', 'deepseek-coder')
        â”‚   â””â”€ RÃ©cupÃ¨re apiKey depuis process.env.DEEPSEEK_API_KEY
        â”‚
        â””â”€ modelInstance.generate(prompt, system)
            â”œâ”€ Construit la requÃªte HTTP
            â”œâ”€ Envoie Ã  https://api.deepseek.com/v1/chat/completions
            â”œâ”€ ReÃ§oit la rÃ©ponse JSON
            â””â”€ Retourne data.choices[0].message.content
    â†“
Code gÃ©nÃ©rÃ© âœ…
```

---

## âš™ï¸ Types TypeScript

```typescript
// lib/modules/llm/types.ts

export interface ModelInfo {
  name: string;              // 'claude-3-5-sonnet-latest'
  label: string;             // 'Claude 3.5 Sonnet (latest)'
  provider: string;          // 'Anthropic'
  maxTokenAllowed: number;   // 8000
}

export interface ProviderInfo {
  name: string;              // 'Anthropic'
  staticModels: ModelInfo[];
  config: ProviderConfig;
  getApiKeyLink?: string;
  labelForGetApiKey?: string;
}

export interface ProviderConfig {
  apiTokenKey: string;       // 'CLAUDE_API_KEY'
  baseUrlKey?: string;
  baseUrl?: string;
}
```

---

## ğŸš¨ Gestion des Erreurs

```typescript
async function safeCallModel(
  provider: string,
  model: string,
  prompt: string
): Promise<string | null> {
  try {
    const manager = LLMManager.getInstance();
    return await manager.callModel(provider, model, prompt);
  } catch (error) {
    if (error instanceof Error) {
      if (error.message.includes('API key')) {
        console.error('âŒ ClÃ© API manquante ou invalide');
      } else if (error.message.includes('401')) {
        console.error('âŒ Authentification Ã©chouÃ©e');
      } else if (error.message.includes('429')) {
        console.error('âŒ Trop de requÃªtes (rate limit)');
      } else {
        console.error('âŒ Erreur:', error.message);
      }
    }
    return null;
  }
}
```

---

## ğŸ“Š Tableau RÃ©capitulatif

| Ã‰lÃ©ment | DÃ©tail |
|--------|--------|
| **Architecture** | Gestionnaire centralisÃ© (Singleton) + Fournisseurs modulaires |
| **Fournisseurs** | Anthropic (Claude), DeepSeek, extensible |
| **Cas d'Utilisation** | Planification, gÃ©nÃ©ration de code, chat, analyse |
| **SÃ©curitÃ©** | ClÃ©s API dans .env.local (jamais en dur) |
| **ExtensibilitÃ©** | Ajouter nouveaux fournisseurs via BaseProvider |
| **Pattern** | Singleton + Abstract Factory + Strategy |

---

## ğŸ¯ Points ClÃ©s

âœ… **Ã€ FAIRE:**
- Utiliser LLMManager.getInstance() pour accÃ©der au gestionnaire
- Configurer les clÃ©s API dans .env.local
- Utiliser Claude pour la planification/analyse
- Utiliser DeepSeek pour la gÃ©nÃ©ration de code
- GÃ©rer les erreurs d'authentification

âŒ **Ã€ Ã‰VITER:**
- Ne pas mettre les clÃ©s API en dur dans le code
- Ne pas crÃ©er plusieurs instances de LLMManager
- Ne pas oublier les messages systÃ¨me (system prompt)
- Ne pas ignorer les limites de tokens

---

## ğŸ”— Ressources

- [Anthropic Claude Docs](https://docs.anthropic.com/)
- [DeepSeek API Docs](https://api-docs.deepseek.com/)
- [Pattern Singleton](https://refactoring.guru/design-patterns/singleton)

